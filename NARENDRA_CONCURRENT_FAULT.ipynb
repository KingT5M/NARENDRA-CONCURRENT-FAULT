{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingT5M/NARENDRA-CONCURRENT-FAULT/blob/main/NARENDRA_CONCURRENT_FAULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from scipy.stats import randint, uniform \n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, LSTM, Flatten, Dense, Dropout, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from kerastuner import HyperModel, Hyperband\n",
        "from keras.layers import Input, Dense, GRU\n",
        "from keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated data shape: (42597, 9)\n",
            "Concatenated data shape after shuffling: (42597, 9)\n"
          ]
        }
      ],
      "source": [
        "# File paths\n",
        "file_paths = [\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\DelayAPP.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\GainRPM.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\HealthyData.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\NoiseAPP.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\PacketLossAPP.csv'\n",
        "]\n",
        "# Load and preprocess the data\n",
        "def load_and_preprocess_data(file_paths):\n",
        "    data_frames = []\n",
        "    for file_path in file_paths:\n",
        "        if os.path.exists(file_path):\n",
        "            data = pd.read_csv(file_path, header=None, usecols=[i for i in range(1, 10)])\n",
        "            data_frames.append(data)\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    if not data_frames:\n",
        "        print(\"No data frames loaded.\")\n",
        "        return None\n",
        "    concatenated_data = pd.concat(data_frames, ignore_index=True)\n",
        "    print(\"Concatenated data shape:\", concatenated_data.shape)\n",
        "    concatenated_data = concatenated_data.sample(frac=1).reset_index(drop=True)\n",
        "    print(\"Concatenated data shape after shuffling:\", concatenated_data.shape)\n",
        "    return concatenated_data\n",
        "\n",
        "# Load and preprocess the data\n",
        "concatenated_data = load_and_preprocess_data(file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the concatenated data\n",
        "def preprocess_data(concatenated_data):\n",
        "    features = concatenated_data.iloc[:, :-1].values\n",
        "    labels = concatenated_data.iloc[:, -1].values\n",
        "    \n",
        "    # Label Encoding\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "    \n",
        "    # One-Hot Encoding\n",
        "    onehot_encoder = OneHotEncoder()\n",
        "    labels_encoded = labels_encoded.reshape(-1, 1)\n",
        "    labels_onehot = onehot_encoder.fit_transform(labels_encoded).toarray()\n",
        "    \n",
        "    return features, labels_onehot\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(features, labels, sequence_length=30):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - sequence_length + 1):\n",
        "        end_ix = i + sequence_length\n",
        "        seq_x, seq_y = features[i:end_ix], labels[end_ix - 1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Preprocess the data\n",
        "features, labels_onehot = preprocess_data(concatenated_data)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "sequence_length = 30\n",
        "X_seq, y_seq = create_sequences(features, labels_onehot, sequence_length)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "426/426 [==============================] - 28s 34ms/step - loss: 875094400.0000 - val_loss: 876027008.0000\n",
            "Epoch 2/50\n",
            "426/426 [==============================] - 10s 24ms/step - loss: 875094720.0000 - val_loss: 876027008.0000\n",
            "Epoch 3/50\n",
            "426/426 [==============================] - 10s 23ms/step - loss: 875094912.0000 - val_loss: 876027008.0000\n",
            "Epoch 4/50\n",
            "426/426 [==============================] - 10s 24ms/step - loss: 875094656.0000 - val_loss: 876027008.0000\n",
            "Epoch 5/50\n",
            "426/426 [==============================] - 16s 36ms/step - loss: 875094720.0000 - val_loss: 876027008.0000\n",
            "Epoch 6/50\n",
            "426/426 [==============================] - 14s 34ms/step - loss: 875094592.0000 - val_loss: 876027008.0000\n",
            "Epoch 7/50\n",
            "426/426 [==============================] - 16s 37ms/step - loss: 875094656.0000 - val_loss: 876027008.0000\n",
            "Epoch 8/50\n",
            "426/426 [==============================] - 14s 32ms/step - loss: 875094784.0000 - val_loss: 876027008.0000\n",
            "Epoch 9/50\n",
            "426/426 [==============================] - 12s 29ms/step - loss: 875095040.0000 - val_loss: 876027008.0000\n",
            "Epoch 10/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875095040.0000 - val_loss: 876027008.0000\n",
            "Epoch 11/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875094592.0000 - val_loss: 876027008.0000\n",
            "Epoch 12/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: 875095040.0000 - val_loss: 876027008.0000\n",
            "Epoch 13/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: 875095552.0000 - val_loss: 876027008.0000\n",
            "Epoch 14/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875094720.0000 - val_loss: 876027008.0000\n",
            "Epoch 15/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875094784.0000 - val_loss: 876027008.0000\n",
            "Epoch 16/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: 875095360.0000 - val_loss: 876027008.0000\n",
            "Epoch 17/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: 875094720.0000 - val_loss: 876027008.0000\n",
            "Epoch 18/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875094912.0000 - val_loss: 876027008.0000\n",
            "Epoch 19/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: 875095104.0000 - val_loss: 876027008.0000\n",
            "Epoch 20/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094784.0000 - val_loss: 876027008.0000\n",
            "Epoch 21/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094592.0000 - val_loss: 876027008.0000\n",
            "Epoch 22/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875095040.0000 - val_loss: 876027008.0000\n",
            "Epoch 23/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: 875095168.0000 - val_loss: 876027008.0000\n",
            "Epoch 24/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875095104.0000 - val_loss: 876027008.0000\n",
            "Epoch 25/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875094784.0000 - val_loss: 876027008.0000\n",
            "Epoch 26/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875094528.0000 - val_loss: 876027008.0000\n",
            "Epoch 27/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: 875094784.0000 - val_loss: 876027008.0000\n",
            "Epoch 28/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875095168.0000 - val_loss: 876027008.0000\n",
            "Epoch 29/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094656.0000 - val_loss: 876027008.0000\n",
            "Epoch 30/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094400.0000 - val_loss: 876027008.0000\n",
            "Epoch 31/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: 875094592.0000 - val_loss: 876027008.0000\n",
            "Epoch 32/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094720.0000 - val_loss: 876027008.0000\n",
            "Epoch 33/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875095040.0000 - val_loss: 876027008.0000\n",
            "Epoch 34/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: 875095104.0000 - val_loss: 876027008.0000\n",
            "Epoch 35/50\n",
            "426/426 [==============================] - 12s 29ms/step - loss: 875095040.0000 - val_loss: 876027008.0000\n",
            "Epoch 36/50\n",
            "426/426 [==============================] - 13s 31ms/step - loss: 875095040.0000 - val_loss: 876027008.0000\n",
            "Epoch 37/50\n",
            "426/426 [==============================] - 14s 34ms/step - loss: 875094656.0000 - val_loss: 876027008.0000\n",
            "Epoch 38/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094592.0000 - val_loss: 876027008.0000\n",
            "Epoch 39/50\n",
            "426/426 [==============================] - 13s 30ms/step - loss: 875095104.0000 - val_loss: 876027008.0000\n",
            "Epoch 40/50\n",
            "426/426 [==============================] - 13s 31ms/step - loss: 875095104.0000 - val_loss: 876027008.0000\n",
            "Epoch 41/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875095104.0000 - val_loss: 876027008.0000\n",
            "Epoch 42/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875094656.0000 - val_loss: 876027008.0000\n",
            "Epoch 43/50\n",
            "426/426 [==============================] - 12s 29ms/step - loss: 875095040.0000 - val_loss: 876027008.0000\n",
            "Epoch 44/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875094784.0000 - val_loss: 876027008.0000\n",
            "Epoch 45/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: 875095360.0000 - val_loss: 876027008.0000\n",
            "Epoch 46/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875095424.0000 - val_loss: 876027008.0000\n",
            "Epoch 47/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: 875094592.0000 - val_loss: 876027008.0000\n",
            "Epoch 48/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094976.0000 - val_loss: 876027008.0000\n",
            "Epoch 49/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094912.0000 - val_loss: 876027008.0000\n",
            "Epoch 50/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: 875094656.0000 - val_loss: 876027008.0000\n",
            "1065/1065 [==============================] - 12s 11ms/step\n",
            "134/134 [==============================] - 2s 10ms/step\n",
            "134/134 [==============================] - 1s 9ms/step\n",
            "1065/1065 [==============================] - 71s 53ms/step\n",
            "134/134 [==============================] - 7s 51ms/step\n",
            "134/134 [==============================] - 7s 51ms/step\n"
          ]
        }
      ],
      "source": [
        "# Denoising Autoencoder (DAE) for feature extraction\n",
        "input_dim = X_train.shape[2]  # Corrected input_dim\n",
        "latent_dim = 64\n",
        "\n",
        "input_layer = Input(shape=(sequence_length, input_dim))\n",
        "encoded = Dense(128, activation='relu')(input_layer)\n",
        "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "dae = Model(input_layer, decoded)\n",
        "dae.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "dae.fit(X_train, X_train, epochs=50, batch_size=64, validation_split=0.2)\n",
        "\n",
        "encoder = Model(input_layer, encoded)\n",
        "features_dae_train = encoder.predict(X_train)\n",
        "features_dae_val = encoder.predict(X_val)\n",
        "features_dae_test = encoder.predict(X_test)\n",
        "\n",
        "features_dae_train = np.array(features_dae_train)\n",
        "features_dae_val = np.array(features_dae_val)\n",
        "features_dae_test = np.array(features_dae_test)\n",
        "\n",
        "# Gated Recurrent Unit (GRU) for feature learning\n",
        "gru_input_shape = features_dae_train.shape[1:]\n",
        "\n",
        "gru_model = Sequential()\n",
        "gru_model.add(GRU(64, input_shape=gru_input_shape, return_sequences=True))\n",
        "gru_model.add(GRU(64, return_sequences=True))\n",
        "gru_model.add(GRU(64, return_sequences=True))\n",
        "gru_model.add(GRU(64))\n",
        "\n",
        "gru_model.add(Reshape((1, 64)))\n",
        "\n",
        "gru_output_train = gru_model.predict(features_dae_train)\n",
        "gru_output_val = gru_model.predict(features_dae_val)\n",
        "gru_output_test = gru_model.predict(features_dae_test)\n",
        "\n",
        "# Define metrics callback\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "        y_true = tf.argmax(y_val, axis=1)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "        print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "        if precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888:\n",
        "            print(\"Achieved desired metrics. Stopping training.\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 2 Complete [00h 01m 29s]\n",
            "val_accuracy: 0.21188630163669586\n",
            "\n",
            "Best val_accuracy So Far: 0.2872915267944336\n",
            "Total elapsed time: 00h 05m 40s\n",
            "\n",
            "Search: Running Trial #3\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "4                 |5                 |cnn_layers\n",
            "4                 |1                 |lstm_layers\n",
            "0                 |3                 |dense_layers\n",
            "558               |781               |epochs\n",
            "1                 |0                 |max_pooling\n",
            "1                 |0                 |dropout\n",
            "0                 |2                 |batch_norm\n",
            "77                |110               |batch_size\n",
            "0.00033805        |0.000837          |learning_rate\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "3                 |3                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n"
          ]
        }
      ],
      "source": [
        "# Define hypermodel\n",
        "class CustomHyperModel(HyperModel):\n",
        "    def __init__(self, X_train, y_train, X_val, y_val):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Hyperparameters\n",
        "        cnn_layers = hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "        lstm_layers = hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "        dense_layers = hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "        epochs = hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "        max_pooling = hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "        dropout = hp.Int('dropout', min_value=0, max_value=2, default=0)\n",
        "        batch_norm = hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "        batch_size = hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "        learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "        # CNN Layers\n",
        "        for i in range(cnn_layers):\n",
        "            model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same', input_shape=(sequence_length, latent_dim)))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Max Pooling Layer\n",
        "        if max_pooling:\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "        # LSTM Layers\n",
        "        for i in range(lstm_layers):\n",
        "            model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Flatten Layer\n",
        "        model.add(Flatten())\n",
        "\n",
        "        # Dense Layers\n",
        "        for i in range(dense_layers):\n",
        "            model.add(Dense(units=64, activation='relu'))\n",
        "            if dropout:\n",
        "                model.add(Dropout(0.5))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Output Layer\n",
        "        model.add(Dense(units=len(fault_types), activation='softmax'))  # Adjusted to match the number of fault types\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel = CustomHyperModel(features_dae_train, y_train, features_dae_val, y_val)\n",
        "\n",
        "\n",
        "# Define the tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='hyperparameters_tuning',\n",
        "    project_name='fault_detection'\n",
        ")\n",
        "\n",
        "# Define metrics callback\n",
        "metrics_callback = MetricsCallback((features_dae_val, y_val))\n",
        "\n",
        "print(type(features_dae_train))\n",
        "print(features_dae_train.shape)\n",
        "\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(\n",
        "    features_dae_train,\n",
        "    y_train,\n",
        "    validation_data=(features_dae_val, y_val),\n",
        "    callbacks=[metrics_callback],\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best hyperparameters found: {best_hps}\")\n",
        "\n",
        "# Build the model with the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model\n",
        "best_model.fit(\n",
        "    features_dae_train,\n",
        "    y_train,\n",
        "    validation_data=(features_dae_val, y_val),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    epochs=best_hps.get('epochs'),\n",
        "    callbacks=[metrics_callback]\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save('C:\\Users\\T5M\\Desktop\\CONCURRENT-FAULT\\GRAPHS & MODEL\\best_model.h5')\n",
        "\n",
        "# Calculate metrics on the test set\n",
        "y_pred = best_model.predict(features_dae_test)\n",
        "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "y_true = tf.argmax(y_test, axis=1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average=None, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true, y_pred_classes, average=None)\n",
        "\n",
        "# Sensor fault types\n",
        "fault_types = ['delay-time', 'gain', 'healthy', 'noise', 'packetloss']\n",
        "\n",
        "# Plotting precision\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(fault_types, precision)\n",
        "plt.title('Precision')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('Precision (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('C:\\Users\\T5M\\Desktop\\CONCURRENT-FAULT\\GRAPHS & MODEL\\precision.png')\n",
        "plt.close()\n",
        "\n",
        "# Plotting recall\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(fault_types, recall)\n",
        "plt.title('Recall')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('Recall (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('C:\\Users\\T5M\\Desktop\\CONCURRENT-FAULT\\GRAPHS & MODEL\\recall.png')\n",
        "plt.close()\n",
        "\n",
        "# Plotting F1-score\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(fault_types, f1)\n",
        "plt.title('F1-score')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('F1-score (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('C:\\Users\\T5M\\Desktop\\CONCURRENT-FAULT\\GRAPHS & MODEL\\f1_score.png')\n",
        "plt.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVnutrJohyyPYjZ76dMzE1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
