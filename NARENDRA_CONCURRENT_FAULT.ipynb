{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingT5M/NARENDRA-CONCURRENT-FAULT/blob/main/NARENDRA_CONCURRENT_FAULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\T5M\\anaconda3\\envs\\Sankara-Ai\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\T5M\\AppData\\Local\\Temp\\ipykernel_9152\\1082931765.py:17: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import HyperModel, Hyperband\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from scipy.stats import randint, uniform \n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, LSTM, Flatten, Dense, Dropout, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from kerastuner import HyperModel, Hyperband\n",
        "from keras.layers import Input, Dense, GRU\n",
        "from keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated data shape: (42597, 9)\n",
            "Concatenated data shape after shuffling: (42597, 9)\n"
          ]
        }
      ],
      "source": [
        "# File paths\n",
        "file_paths = [\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\DelayAPP.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\GainRPM.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\HealthyData.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\NoiseAPP.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\PacketLossAPP.csv'\n",
        "]\n",
        "# Load and preprocess the data\n",
        "def load_and_preprocess_data(file_paths):\n",
        "    data_frames = []\n",
        "    for file_path in file_paths:\n",
        "        if os.path.exists(file_path):\n",
        "            data = pd.read_csv(file_path, header=None, usecols=[i for i in range(1, 10)])\n",
        "            data_frames.append(data)\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    if not data_frames:\n",
        "        print(\"No data frames loaded.\")\n",
        "        return None\n",
        "    concatenated_data = pd.concat(data_frames, ignore_index=True)\n",
        "    print(\"Concatenated data shape:\", concatenated_data.shape)\n",
        "    concatenated_data = concatenated_data.sample(frac=1).reset_index(drop=True)\n",
        "    print(\"Concatenated data shape after shuffling:\", concatenated_data.shape)\n",
        "    return concatenated_data\n",
        "\n",
        "# Load and preprocess the data\n",
        "concatenated_data = load_and_preprocess_data(file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the concatenated data\n",
        "def preprocess_data(concatenated_data):\n",
        "    features = concatenated_data.iloc[:, :-1].values\n",
        "    labels = concatenated_data.iloc[:, -1].values\n",
        "    \n",
        "    # Label Encoding\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "    \n",
        "    # One-Hot Encoding\n",
        "    onehot_encoder = OneHotEncoder()\n",
        "    labels_encoded = labels_encoded.reshape(-1, 1)\n",
        "    labels_onehot = onehot_encoder.fit_transform(labels_encoded).toarray()\n",
        "    \n",
        "    return features, labels_onehot\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(features, labels, sequence_length=30):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - sequence_length + 1):\n",
        "        end_ix = i + sequence_length\n",
        "        seq_x, seq_y = features[i:end_ix], labels[end_ix - 1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Preprocess the data\n",
        "features, labels_onehot = preprocess_data(concatenated_data)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "sequence_length = 30\n",
        "X_seq, y_seq = create_sequences(features, labels_onehot, sequence_length)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 4 Complete [00h 01m 19s]\n",
            "val_accuracy: 0.2356119304895401\n",
            "\n",
            "Best val_accuracy So Far: 0.2356119304895401\n",
            "Total elapsed time: 00h 21m 22s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Search: Running Trial #5\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "4                 |3                 |cnn_layers\n",
            "2                 |1                 |lstm_layers\n",
            "1                 |3                 |dense_layers\n",
            "630               |119               |epochs\n",
            "0                 |1                 |max_pooling\n",
            "1                 |0                 |dropout\n",
            "2                 |0                 |batch_norm\n",
            "127               |88                |batch_size\n",
            "0.00037584        |0.00098326        |learning_rate\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "3                 |3                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            " 205/1065 [====>.........................] - ETA: 1:31 - loss: 1.9718 - accuracy: 0.2218"
          ]
        }
      ],
      "source": [
        "# Denoising Autoencoder (DAE) for feature extraction\n",
        "input_dim = X_train.shape[2]\n",
        "latent_dim = 64\n",
        "\n",
        "input_layer = Input(shape=(sequence_length, input_dim))\n",
        "encoded = Dense(128, activation='relu')(input_layer)\n",
        "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "dae = Model(input_layer, decoded)\n",
        "dae.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "dae.fit(X_train, X_train, epochs=50, batch_size=64, validation_split=0.2)\n",
        "\n",
        "encoder = Model(input_layer, encoded)\n",
        "features_dae_train = encoder.predict(X_train)\n",
        "features_dae_val = encoder.predict(X_val)\n",
        "features_dae_test = encoder.predict(X_test)\n",
        "\n",
        "features_dae_train = np.array(features_dae_train)\n",
        "features_dae_val = np.array(features_dae_val)\n",
        "features_dae_test = np.array(features_dae_test)\n",
        "\n",
        "# Gated Recurrent Unit (GRU) for feature learning\n",
        "gru_input_shape = features_dae_train.shape[1:]\n",
        "\n",
        "gru_model = Sequential()\n",
        "gru_model.add(GRU(64, input_shape=gru_input_shape, return_sequences=True))\n",
        "gru_model.add(GRU(64, return_sequences=True))\n",
        "gru_model.add(GRU(64, return_sequences=True))\n",
        "gru_model.add(GRU(64))\n",
        "\n",
        "gru_model.add(Reshape((1, 64)))\n",
        "\n",
        "gru_output_train = gru_model.predict(features_dae_train)\n",
        "gru_output_val = gru_model.predict(features_dae_val)\n",
        "gru_output_test = gru_model.predict(features_dae_test)\n",
        "\n",
        "# Define metrics callback\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.precision_history = []\n",
        "        self.recall_history = []\n",
        "        self.f1_history = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "        print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "        if precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888:\n",
        "            print(\"Achieved desired metrics. Stopping training.\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "        self.precision_history.append(precision)\n",
        "        self.recall_history.append(recall)\n",
        "        self.f1_history.append(f1)\n",
        "\n",
        "# Define fault_types\n",
        "fault_types = ['delay-time', 'gain', 'healthy', 'noise', 'packetloss']\n",
        "\n",
        "# Define hypermodel\n",
        "class CustomHyperModel(HyperModel):\n",
        "    def __init__(self, X_train, y_train, X_val, y_val, fault_types):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.fault_types = fault_types  \n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        cnn_layers = hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "        lstm_layers = hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "        dense_layers = hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "        epochs = hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "        max_pooling = hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "        dropout = hp.Int('dropout', min_value=0, max_value=2, default=0)\n",
        "        batch_norm = hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "        batch_size = hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "        learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "        for i in range(cnn_layers):\n",
        "            model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same', input_shape=(sequence_length, latent_dim)))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        if max_pooling:\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "        for i in range(lstm_layers):\n",
        "            model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Flatten())\n",
        "\n",
        "        for i in range(dense_layers):\n",
        "            model.add(Dense(units=64, activation='relu'))\n",
        "            if dropout:\n",
        "                model.add(Dropout(0.5))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Dense(units=len(fault_types), activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel = CustomHyperModel(features_dae_train, y_train, features_dae_val, y_val, fault_types)\n",
        "\n",
        "# Define the tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='hyperparameters_tuning',\n",
        "    project_name='fault_detection'\n",
        ")\n",
        "\n",
        "# Define metrics callback\n",
        "metrics_callback = MetricsCallback((features_dae_val, y_val))\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(\n",
        "    features_dae_train,\n",
        "    y_train,\n",
        "    validation_data=(features_dae_val, y_val),\n",
        "    callbacks=[metrics_callback],\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best hyperparameters found: {best_hps}\")\n",
        "\n",
        "# Build the model with the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model\n",
        "best_model.fit(\n",
        "    features_dae_train,\n",
        "    y_train,\n",
        "    validation_data=(features_dae_val, y_val),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    epochs=best_hps.get('epochs'),\n",
        "    callbacks=[metrics_callback]\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(r'C:\\Users\\T5M\\Desktop\\CONCURRENT-FAULT\\best_model')\n",
        "\n",
        "# Calculate metrics on the test set\n",
        "y_pred = best_model.predict(features_dae_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average=None, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true, y_pred_classes, average=None)\n",
        "\n",
        "# Plotting precision, recall, and F1-score in one graph\n",
        "x = np.arange(len(fault_types))\n",
        "width = 0.2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rects1 = ax.bar(x - width, precision, width, label='Precision')\n",
        "rects2 = ax.bar(x, recall, width, label='Recall')\n",
        "rects3 = ax.bar(x + width, f1, width, label='F1-score')\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Scores by fault types')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(fault_types)\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "# Save the graph\n",
        "plt.savefig(r'C:\\Users\\T5M\\Desktop\\CONCURRENT-FAULT\\graphs\\scores.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVnutrJohyyPYjZ76dMzE1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
