{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingT5M/NARENDRA-CONCURRENT-FAULT/blob/main/NARENDRA_CONCURRENT_FAULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from scipy.stats import randint, uniform \n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, LSTM, Flatten, Dense, Dropout, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from kerastuner import HyperModel, Hyperband\n",
        "from keras.layers import Input, Dense, GRU\n",
        "from keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# File paths\n",
        "file_paths = [\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\DelayAPP.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\GainRPM.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\HealthyData.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\NoiseAPP.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\PacketLossAPP.csv'\n",
        "]\n",
        "\n",
        "# Fault types\n",
        "fault_types = ['delay-time', 'gain', 'healthy', 'noise', 'packetloss']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated data shape: (42592, 40)\n",
            "Concatenated data shape after shuffling: (42592, 40)\n",
            "Number of features: 42592\n",
            "Number of labels: 42592\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the data\n",
        "def load_and_preprocess_data(file_paths, fault_types):\n",
        "    data_frames = []\n",
        "    for file_path in file_paths:\n",
        "        if os.path.exists(file_path):\n",
        "            data_frames.append(pd.read_csv(file_path))\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    if not data_frames:\n",
        "        print(\"No data frames loaded.\")\n",
        "        return None, None\n",
        "    concatenated_data = pd.concat(data_frames)\n",
        "    print(\"Concatenated data shape:\", concatenated_data.shape)\n",
        "    concatenated_data = concatenated_data.sample(frac=1).reset_index(drop=True)\n",
        "    print(\"Concatenated data shape after shuffling:\", concatenated_data.shape)\n",
        "    features = concatenated_data.iloc[:, 1:9].values\n",
        "    labels = concatenated_data.iloc[:, 9].values\n",
        "    print(\"Number of features:\", len(features))\n",
        "    print(\"Number of labels:\", len(labels))\n",
        "    if len(features) == 0 or len(labels) == 0:\n",
        "        print(\"No samples found after preprocessing.\")\n",
        "        return None, None\n",
        "    \n",
        "    # Label Encoding\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(labels)\n",
        "    \n",
        "    # One-Hot Encoding\n",
        "    onehot_encoder = OneHotEncoder()\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    labels_onehot = onehot_encoder.fit_transform(integer_encoded).toarray()\n",
        "    \n",
        "    # Ensure that the labels are in the same order as fault_types\n",
        "    label_encoder.classes_ = np.array(fault_types)\n",
        "    \n",
        "    return features, labels_onehot\n",
        "\n",
        "def create_sequences(features, labels, sequence_length=30):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - sequence_length + 1):\n",
        "        end_ix = i + sequence_length\n",
        "        seq_x, seq_y = features[i:end_ix], labels[i:end_ix][-1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Load and preprocess the data\n",
        "features, labels = load_and_preprocess_data(file_paths, fault_types)\n",
        "sequence_length = 30\n",
        "X_seq, y_seq = create_sequences(features, labels, sequence_length)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "426/426 [==============================] - 29s 32ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/50\n",
            "426/426 [==============================] - 16s 38ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/50\n",
            "426/426 [==============================] - 15s 36ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/50\n",
            "426/426 [==============================] - 13s 31ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/50\n",
            "426/426 [==============================] - 14s 33ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/50\n",
            "426/426 [==============================] - 12s 29ms/step - loss: nan - val_loss: nan\n",
            "Epoch 11/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 12/50\n",
            "426/426 [==============================] - 13s 31ms/step - loss: nan - val_loss: nan\n",
            "Epoch 13/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: nan - val_loss: nan\n",
            "Epoch 14/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 15/50\n",
            "426/426 [==============================] - 19s 44ms/step - loss: nan - val_loss: nan\n",
            "Epoch 16/50\n",
            "426/426 [==============================] - 12s 29ms/step - loss: nan - val_loss: nan\n",
            "Epoch 17/50\n",
            "426/426 [==============================] - 11s 27ms/step - loss: nan - val_loss: nan\n",
            "Epoch 18/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 19/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 20/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 21/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 22/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 23/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 24/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 25/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: nan - val_loss: nan\n",
            "Epoch 26/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 27/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 28/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 29/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 30/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 31/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 32/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 33/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 34/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 35/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: nan - val_loss: nan\n",
            "Epoch 36/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: nan - val_loss: nan\n",
            "Epoch 37/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 38/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 39/50\n",
            "426/426 [==============================] - 11s 25ms/step - loss: nan - val_loss: nan\n",
            "Epoch 40/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: nan - val_loss: nan\n",
            "Epoch 41/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 42/50\n",
            "426/426 [==============================] - 12s 27ms/step - loss: nan - val_loss: nan\n",
            "Epoch 43/50\n",
            "426/426 [==============================] - 11s 26ms/step - loss: nan - val_loss: nan\n",
            "Epoch 44/50\n",
            "426/426 [==============================] - 12s 28ms/step - loss: nan - val_loss: nan\n",
            "Epoch 45/50\n",
            "426/426 [==============================] - 14s 33ms/step - loss: nan - val_loss: nan\n",
            "Epoch 46/50\n",
            "426/426 [==============================] - 10s 24ms/step - loss: nan - val_loss: nan\n",
            "Epoch 47/50\n",
            "426/426 [==============================] - 14s 32ms/step - loss: nan - val_loss: nan\n",
            "Epoch 48/50\n",
            "426/426 [==============================] - 14s 34ms/step - loss: nan - val_loss: nan\n",
            "Epoch 49/50\n",
            "426/426 [==============================] - 14s 32ms/step - loss: nan - val_loss: nan\n",
            "Epoch 50/50\n",
            "426/426 [==============================] - 13s 31ms/step - loss: nan - val_loss: nan\n",
            "1065/1065 [==============================] - 14s 12ms/step\n",
            "133/133 [==============================] - 2s 11ms/step\n",
            "134/134 [==============================] - 2s 12ms/step\n",
            "1065/1065 [==============================] - 75s 57ms/step\n",
            "133/133 [==============================] - 8s 56ms/step\n",
            "134/134 [==============================] - 8s 60ms/step\n"
          ]
        }
      ],
      "source": [
        "# Denoising Autoencoder (DAE) for feature extraction\n",
        "input_dim = X_train.shape[2]\n",
        "latent_dim = 64\n",
        "\n",
        "input_layer = Input(shape=(sequence_length, input_dim))\n",
        "encoded = Dense(128, activation='relu')(input_layer)\n",
        "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "dae = Model(input_layer, decoded)\n",
        "dae.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "dae.fit(X_train, X_train, epochs=50, batch_size=64, validation_split=0.2)\n",
        "\n",
        "encoder = Model(input_layer, encoded)\n",
        "features_dae_train = encoder.predict(X_train)\n",
        "features_dae_val = encoder.predict(X_val)\n",
        "features_dae_test = encoder.predict(X_test)\n",
        "\n",
        "features_dae_train = np.array(features_dae_train)\n",
        "features_dae_val = np.array(features_dae_val)\n",
        "features_dae_test = np.array(features_dae_test)\n",
        "\n",
        "# Gated Recurrent Unit (GRU) for feature learning\n",
        "gru_input_shape = features_dae_train.shape[1:]\n",
        "\n",
        "gru_model = Sequential()\n",
        "gru_model.add(GRU(64, input_shape=gru_input_shape, return_sequences=True))\n",
        "gru_model.add(GRU(64, return_sequences=True))\n",
        "gru_model.add(GRU(64, return_sequences=True))\n",
        "gru_model.add(GRU(64))\n",
        "\n",
        "gru_model.add(Reshape((1, 64)))\n",
        "\n",
        "gru_output_train = gru_model.predict(features_dae_train)\n",
        "gru_output_val = gru_model.predict(features_dae_val)\n",
        "gru_output_test = gru_model.predict(features_dae_test)\n",
        "\n",
        "# Define metrics callback\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "        y_true = tf.argmax(y_val, axis=1)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "        print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "        if precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888:\n",
        "            print(\"Achieved desired metrics. Stopping training.\")\n",
        "            self.model.stop_training = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 11 Complete [00h 09m 08s]\n",
            "val_accuracy: 0.8038063645362854\n",
            "\n",
            "Best val_accuracy So Far: 0.8038063645362854\n",
            "Total elapsed time: 01h 10m 10s\n",
            "\n",
            "Search: Running Trial #12\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "0                 |4                 |cnn_layers\n",
            "2                 |4                 |lstm_layers\n",
            "4                 |4                 |dense_layers\n",
            "178               |447               |epochs\n",
            "1                 |0                 |max_pooling\n",
            "0                 |2                 |dropout\n",
            "0                 |0                 |batch_norm\n",
            "145               |76                |batch_size\n",
            "0.00065937        |0.00064271        |learning_rate\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "3                 |3                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            " 963/1065 [==========================>...] - ETA: 5s - loss: 0.5796 - accuracy: 0.8005"
          ]
        }
      ],
      "source": [
        "# Define hypermodel\n",
        "class CustomHyperModel(HyperModel):\n",
        "    def __init__(self, X_train, y_train, X_val, y_val):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Hyperparameters\n",
        "        cnn_layers = hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "        lstm_layers = hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "        dense_layers = hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "        epochs = hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "        max_pooling = hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "        dropout = hp.Int('dropout', min_value=0, max_value=2, default=0)\n",
        "        batch_norm = hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "        batch_size = hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "        learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "        # CNN Layers\n",
        "        for i in range(cnn_layers):\n",
        "            model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same', input_shape=(30, 64)))  # Adjusted input shape\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Max Pooling Layer\n",
        "        if max_pooling:\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "        # LSTM Layers\n",
        "        for i in range(lstm_layers):\n",
        "            model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Flatten Layer\n",
        "        model.add(Flatten())\n",
        "\n",
        "        # Dense Layers\n",
        "        for i in range(dense_layers):\n",
        "            model.add(Dense(units=64, activation='relu'))\n",
        "            if dropout:\n",
        "                model.add(Dropout(0.5))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        # Output Layer\n",
        "        model.add(Dense(units=2, activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel = CustomHyperModel(features_dae_train, y_train, features_dae_val, y_val)\n",
        "\n",
        "\n",
        "# Define the tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='hyperparameters_tuning',\n",
        "    project_name='fault_detection'\n",
        ")\n",
        "\n",
        "# Define metrics callback\n",
        "metrics_callback = MetricsCallback((features_dae_val, y_val))\n",
        "\n",
        "print(type(features_dae_train))\n",
        "print(features_dae_train.shape)\n",
        "\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(\n",
        "    features_dae_train,\n",
        "    y_train,\n",
        "    validation_data=(features_dae_val, y_val),\n",
        "    callbacks=[metrics_callback],\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best hyperparameters found: {best_hps}\")\n",
        "\n",
        "# Build the model with the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the model\n",
        "best_model.fit(\n",
        "    features_dae_train,\n",
        "    y_train,\n",
        "    validation_data=(features_dae_val, y_val),\n",
        "    batch_size=best_hps.get('batch_size'),\n",
        "    epochs=best_hps.get('epochs'),\n",
        "    callbacks=[metrics_callback]\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save('best_model.h5')\n",
        "\n",
        "# Calculate metrics on the test set\n",
        "y_pred = best_model.predict(features_dae_test)\n",
        "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "y_true = tf.argmax(y_test, axis=1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average=None, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true, y_pred_classes, average=None)\n",
        "\n",
        "# Sensor fault types\n",
        "fault_types = ['delay-time', 'gain', 'healthy', 'noise', 'packetloss']\n",
        "\n",
        "# Plotting precision\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(fault_types, precision)\n",
        "plt.title('Precision')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('Precision (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('precision.png')\n",
        "plt.close()\n",
        "\n",
        "# Plotting recall\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(fault_types, recall)\n",
        "plt.title('Recall')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('Recall (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('recall.png')\n",
        "plt.close()\n",
        "\n",
        "# Plotting F1-score\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(fault_types, f1)\n",
        "plt.title('F1-score')\n",
        "plt.xlabel('Fault Types')\n",
        "plt.ylabel('F1-score (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('f1_score.png')\n",
        "plt.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVnutrJohyyPYjZ76dMzE1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
