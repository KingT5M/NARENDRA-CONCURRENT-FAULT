{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingT5M/NARENDRA-CONCURRENT-FAULT/blob/main/NARENDRA_CONCURRENT_FAULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\T5M\\anaconda3\\envs\\Sankara-Ai\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\T5M\\AppData\\Local\\Temp\\ipykernel_9460\\3921900960.py:17: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import HyperModel, HyperParameters\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated data shape: (42597, 9)\n",
            "Concatenated data shape after shuffling: (42597, 9)\n",
            "WARNING:tensorflow:From c:\\Users\\T5M\\anaconda3\\envs\\Sankara-Ai\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\T5M\\anaconda3\\envs\\Sankara-Ai\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From c:\\Users\\T5M\\anaconda3\\envs\\Sankara-Ai\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "426/426 [==============================] - 7s 12ms/step - loss: 875072640.0000 - val_loss: 875738112.0000\n",
            "Epoch 2/50\n",
            "426/426 [==============================] - 5s 11ms/step - loss: 875073152.0000 - val_loss: 875738112.0000\n",
            "Epoch 3/50\n",
            "426/426 [==============================] - 7s 15ms/step - loss: 875072960.0000 - val_loss: 875738112.0000\n",
            "Epoch 4/50\n",
            "426/426 [==============================] - 6s 15ms/step - loss: 875073024.0000 - val_loss: 875738112.0000\n",
            "Epoch 5/50\n",
            "108/426 [======>.......................] - ETA: 3s - loss: 875006784.0000"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from scipy.stats import randint, uniform \n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, BatchNormalization, MaxPooling1D, LSTM, Flatten, Dense, Dropout, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from kerastuner import HyperModel, HyperParameters\n",
        "from keras.layers import Input, Dense, GRU\n",
        "from keras.models import Model\n",
        "\n",
        "# File paths\n",
        "file_paths = [\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\DelayAPP.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\GainRPM.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\HealthyData.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\NoiseAPP.csv',\n",
        "    'C:\\\\Users\\\\T5M\\\\Desktop\\\\CONCURRENT-FAULT\\\\DATASET\\\\PacketLossAPP.csv'\n",
        "]\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_and_preprocess_data(file_paths):\n",
        "    data_frames = []\n",
        "    for file_path in file_paths:\n",
        "        if os.path.exists(file_path):\n",
        "            data = pd.read_csv(file_path, header=None, usecols=[i for i in range(1, 10)])\n",
        "            data_frames.append(data)\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    if not data_frames:\n",
        "        print(\"No data frames loaded.\")\n",
        "        return None\n",
        "    concatenated_data = pd.concat(data_frames, ignore_index=True)\n",
        "    print(\"Concatenated data shape:\", concatenated_data.shape)\n",
        "    concatenated_data = concatenated_data.sample(frac=1).reset_index(drop=True)\n",
        "    print(\"Concatenated data shape after shuffling:\", concatenated_data.shape)\n",
        "    return concatenated_data\n",
        "\n",
        "# Load and preprocess the data\n",
        "concatenated_data = load_and_preprocess_data(file_paths)\n",
        "\n",
        "# Preprocess the concatenated data\n",
        "def preprocess_data(concatenated_data):\n",
        "    features = concatenated_data.iloc[:, :-1].values\n",
        "    labels = concatenated_data.iloc[:, -1].values\n",
        "    \n",
        "    # Label Encoding\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels_encoded = label_encoder.fit_transform(labels)\n",
        "    \n",
        "    # One-Hot Encoding\n",
        "    onehot_encoder = OneHotEncoder()\n",
        "    labels_encoded = labels_encoded.reshape(-1, 1)\n",
        "    labels_onehot = onehot_encoder.fit_transform(labels_encoded).toarray()\n",
        "    \n",
        "    return features, labels_onehot\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(features, labels, sequence_length=30):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - sequence_length + 1):\n",
        "        end_ix = i + sequence_length\n",
        "        seq_x, seq_y = features[i:end_ix], labels[end_ix - 1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Preprocess the data\n",
        "features, labels_onehot = preprocess_data(concatenated_data)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "sequence_length = 30\n",
        "X_seq, y_seq = create_sequences(features, labels_onehot, sequence_length)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Denoising Autoencoder (DAE) for feature extraction\n",
        "input_dim = X_train.shape[2]\n",
        "latent_dim = 64\n",
        "\n",
        "input_layer = Input(shape=(sequence_length, input_dim))\n",
        "encoded = Dense(128, activation='relu')(input_layer)\n",
        "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "dae = Model(input_layer, decoded)\n",
        "dae.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "dae.fit(X_train, X_train, epochs=50, batch_size=64, validation_split=0.2)\n",
        "\n",
        "encoder = Model(input_layer, encoded)\n",
        "features_dae_train = encoder.predict(X_train)\n",
        "features_dae_val = encoder.predict(X_val)\n",
        "features_dae_test = encoder.predict(X_test)\n",
        "\n",
        "features_dae_train = np.array(features_dae_train)\n",
        "features_dae_val = np.array(features_dae_val)\n",
        "features_dae_test = np.array(features_dae_test)\n",
        "\n",
        "# Gated Recurrent Unit (GRU) for feature learning\n",
        "gru_input_shape = features_dae_train.shape[1:]\n",
        "\n",
        "gru_model = Sequential()\n",
        "gru_model.add(GRU(64, input_shape=gru_input_shape, return_sequences=True))\n",
        "gru_model.add(GRU(64, return_sequences=True))\n",
        "gru_model.add(GRU(64, return_sequences=True))\n",
        "gru_model.add(GRU(64))\n",
        "\n",
        "gru_model.add(Reshape((1, 64)))\n",
        "\n",
        "gru_output_train = gru_model.predict(features_dae_train)\n",
        "gru_output_val = gru_model.predict(features_dae_val)\n",
        "gru_output_test = gru_model.predict(features_dae_test)\n",
        "\n",
        "# Define metrics callback with REINFORCE algorithm\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self, validation_data, fault_types):\n",
        "        super(MetricsCallback, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.fault_types = fault_types\n",
        "        self.rewards = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
        "        f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "\n",
        "        # Calculate reward based on achieved metrics\n",
        "        reward = (precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888) - 1\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "        print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "        if precision >= 0.9886 and recall >= 0.9890 and f1 >= 0.9888:\n",
        "            print(\"Achieved desired metrics. Stopping training.\")\n",
        "            self.model.stop_training = True\n",
        "            \n",
        "# Define fault_types\n",
        "fault_types = ['delay-time', 'gain', 'healthy', 'noise', 'packetloss']\n",
        "\n",
        "# Define hypermodel with REINFORCE\n",
        "class CustomHyperModel(HyperParameters):\n",
        "    def __init__(self, X_train, y_train, X_val, y_val, fault_types):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.fault_types = fault_types  \n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        \n",
        "        cnn_layers = hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "        lstm_layers = hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "        dense_layers = hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "        epochs = hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "        max_pooling = hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "        dropout = hp.Float('dropout', min_value=0, max_value=0.5, default=0)\n",
        "        batch_norm = hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "        batch_size = hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "        learning_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "        for i in range(cnn_layers):\n",
        "            model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same', input_shape=(sequence_length, latent_dim)))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        if max_pooling:\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "        for i in range(lstm_layers):\n",
        "            model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Flatten())\n",
        "\n",
        "        for i in range(dense_layers):\n",
        "            model.add(Dense(units=64, activation='relu'))\n",
        "            if dropout:\n",
        "                model.add(Dropout(dropout))\n",
        "            if batch_norm:\n",
        "                model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Dense(units=len(self.fault_types), activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "# Define parameters\n",
        "num_episodes = 100\n",
        "max_steps_per_episode = 50\n",
        "current_hyperparameters = {\n",
        "    'cnn_layers': 5,\n",
        "    'lstm_layers': 4,\n",
        "    'dense_layers': 0,\n",
        "    'epochs': 850,\n",
        "    'max_pooling': 1,\n",
        "    'dropout': 0,\n",
        "    'batch_norm': 2,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.0005\n",
        "}\n",
        "\n",
        "print(\"Reinforcement Learning starts.\")\n",
        "# Training loop with REINFORCE\n",
        "for episode in range(num_episodes):\n",
        "    episode_rewards = []\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        sampled_hyperparameters = {\n",
        "            'cnn_layers': int(np.random.normal(current_hyperparameters['cnn_layers'], 0.1)),\n",
        "            'lstm_layers': int(np.random.normal(current_hyperparameters['lstm_layers'], 0.1)),\n",
        "            'dense_layers': int(np.random.normal(current_hyperparameters['dense_layers'], 0.1)),\n",
        "            'epochs': int(np.random.normal(current_hyperparameters['epochs'], 0.1)),\n",
        "            'max_pooling': int(np.random.normal(current_hyperparameters['max_pooling'], 0.1)),\n",
        "            'dropout': int(np.random.normal(current_hyperparameters['dropout'], 0.1)),\n",
        "            'batch_norm': int(np.random.normal(current_hyperparameters['batch_norm'], 0.1)),\n",
        "            'batch_size': int(np.random.normal(current_hyperparameters['batch_size'], 0.1)),\n",
        "            'learning_rate': np.random.normal(current_hyperparameters['learning_rate'], 0.1)\n",
        "        }\n",
        "        hp = HyperParameters()\n",
        "        # Define hyper-parameter ranges\n",
        "        hp.Int('cnn_layers', min_value=0, max_value=5, default=5)\n",
        "        hp.Int('lstm_layers', min_value=0, max_value=5, default=4)\n",
        "        hp.Int('dense_layers', min_value=0, max_value=5, default=0)\n",
        "        hp.Int('epochs', min_value=50, max_value=900, default=850)\n",
        "        hp.Int('max_pooling', min_value=0, max_value=1, default=1)\n",
        "        hp.Float('dropout', min_value=0, max_value=0.5, default=0)\n",
        "        hp.Int('batch_norm', min_value=0, max_value=2, default=2)\n",
        "        hp.Int('batch_size', min_value=64, max_value=150, default=64)\n",
        "        hp.Float('learning_rate', min_value=0.0001, max_value=0.001, default=0.0005, sampling='linear')\n",
        "\n",
        "        # Pass the HyperParameters object to the CustomHyperModel\n",
        "        hypermodel = CustomHyperModel(features_dae_train, y_train, features_dae_val, y_val, fault_types)\n",
        "\n",
        "        # Build the model with sampled hyperparameters\n",
        "        model = hypermodel.build(hp)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(\n",
        "            features_dae_train,\n",
        "            y_train,\n",
        "            validation_data=(features_dae_val, y_val),\n",
        "            batch_size=sampled_hyperparameters['batch_size'],\n",
        "            epochs=sampled_hyperparameters['epochs'],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        loss, accuracy = model.evaluate(features_dae_val, y_val, verbose=0)\n",
        "\n",
        "        # Calculate reward based on accuracy\n",
        "        reward = accuracy - 0.5  # Example reward function, you can adjust this based on your specific objectives\n",
        "\n",
        "        # Append reward to episode rewards\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "    # Update hyperparameters using REINFORCE algorithm\n",
        "    for hp_name, reward in zip(current_hyperparameters.keys(), episode_rewards):\n",
        "        current_hyperparameters[hp_name] += 0.01 * reward  # Adjust the update factor as needed\n",
        "\n",
        "    print(f\"Episode {episode + 1}: Rewards - {np.mean(episode_rewards)}\")\n",
        "\n",
        "print(\"Reinforcement Learning stops.\")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hyperparameters = current_hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "\n",
        "# Build the final model with the best hyperparameters\n",
        "best_hypermodel = CustomHyperModel(features_dae_train, y_train, features_dae_val, y_val, fault_types)\n",
        "best_model = best_hypermodel.build(best_hyperparameters)\n",
        "\n",
        "# Train the final model\n",
        "history = best_model.fit(\n",
        "    features_dae_train,\n",
        "    y_train,\n",
        "    validation_data=(features_dae_val, y_val),\n",
        "    batch_size=best_hyperparameters['batch_size'],\n",
        "    epochs=best_hyperparameters['epochs'],\n",
        "    callbacks=[MetricsCallback((features_dae_val, y_val), fault_types)]\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save('best_model.h5')\n",
        "\n",
        "# Calculate metrics on the test set\n",
        "y_pred = best_model.predict(features_dae_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average=None, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred_classes, average=None)\n",
        "f1 = f1_score(y_true, y_pred_classes, average=None)\n",
        "\n",
        "# Plotting precision, recall, and F1-score in one graph\n",
        "x = np.arange(len(fault_types))\n",
        "width = 0.2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rects1 = ax.bar(x - width, precision, width, label='Precision')\n",
        "rects2 = ax.bar(x, recall, width, label='Recall')\n",
        "rects3 = ax.bar(x + width, f1, width, label='F1-score')\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Scores by fault types')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(fault_types)\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "# Save the graph\n",
        "plt.savefig('scores.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVnutrJohyyPYjZ76dMzE1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
